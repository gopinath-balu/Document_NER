{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "from tika import parser\n",
    "\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import spacy\n",
    "from thinc.api import SGD\n",
    "from spacy.tokens import DocBin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf_file = '/Users/gopinath.balu/Downloads/text_extraction/data/Use caes NLP.pdf'\n",
    "pdf_file = '/home/admin/gopi/Workspace/text_extraction/data/Use caes NLP.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = parser.from_file(pdf_file)\n",
    "content = raw['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_newline = lambda x: x.replace('\\n', ' ').strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_content = remove_newline(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "edited_content = \"Numéro de TVA : BE 0425.888.396 RPR GENT KBC IBAN BE50 4459 6389 4118 BIC KREDBEBB  ING IBAN BE98 3900 3232 4293 BIC BBRUBEBB  BNP IBAN BE78 2900 1606 0086 BIC GEBABEBB  BEL IBAN BE30 5645 1378 2011 BIC GKCCBEBB 1 / 2  pgb-Europe nv  Gontrode Heirweg 170  9090 Melle - Belgium  Tél. +32 (0)9 272 70 70  info@pgb-europe.com  Fax +32 (0)9 272 70 99  www.pgb-europe.com  Société Certifiée ISO 9001  CONFIRMATION D'ORDRE - 301437  Date du document 02/05/2022 Numéro de client 1000293 Numéro de TVA client BE0415809702 Votre réf. 13639 E-mail florine.fernez@plasticentre.b  e Tél. 069532060  Plasticentre sa  Lid Menouquin :115  /PLH   Chée de Russeignies 11  9600 Renaix  Adresse de facturation Menouquin sv Pro  Groep Meno0  Vieux Chemin de Thines 14  1400 Nivelles  Adresse de livraison Plasticentre  003147999  Rue De L'Artisanat 11  7900 Leuze-en-Hainaut  Vous pourrez trouver nos conditions générales de vente et de livraison sur ce  document ou via le lien suivant :   https://www.pgb-europe.com/fr-fr/legal/terms-and-conditions  Conditions de paiement Dû en 60 jours Conditions de livraison CPT (Port payé) Leuze-en-Hainaut Valeur d'envoi franco 250€     POS. DESCRIPTION QUANTITE   COMMANDEE QUANTITE   CONFIRMEE PRIX /  UNITE  NET /  UNITE  MONTANT  NET  10  PFWVTG001005000803 / PWVTGG50080 Z (*) Vis à panneaux PFS+ TF-T * 5,00x80 Zn (T25)  double tête plate six lobes int. avec nervures  filet partiel - pointe coupante T17  emballage standard QTE: 200 Pce  Demandé: 2.000 Pce Arrondi:  2.400 Pce  Stock: 2.400 Pce  Pce  -2 %  Pce  117,53 €  20  PFWVTV001004500603 / PWVTVK45060 Z (*) Vis à panneaux PFS+ TF-T * 4,50x60 Zn (T20)  double tête plate six lobes int. avec nervures  filet total - pointe coupante T17  emballage petit QTE: 200 Pce  2.400 Pce Stock: 2.400 Pce   Pce   Pce  68,71 €  Sous Total 188,64 €  Remise cumulée -2,40 €  Montant net 186,24 €  TVA (21%) 39,11 €  Montant total 225,35 €  Poids brut 27,600 KG CONFIRMATION D'ORDRE - 301437  Date de document 02/05/2022  2 / 2  Nous restons à votre disposition pour tout renseignement complémentaire.  Sofie Van Dorpe Sales sofie.van.dorpe@pgb-europe.com +32 92727072  (*) Sans Chrome VI conformément à la directive RoHS (directive 2011/65 /UE).\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#items_to_extract = ['Art. client: 924734', '49,97 €  / 1.000', '48,97 €  / 1.000', 'Art. client: 929735', '28,63 €  / 1.000', '28,63 €  / 1.000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "art_clients = [f'{random.randint(10000, 999999)}' for i in range(100)]\n",
    "prices1 = [f'{random.randint(1, 999)},{random.randint(0, 99)} / 1.000' for i in range(100)]\n",
    "prices2 = [f'{random.randint(1, 999)},{random.randint(0, 99)} / 1.000' for i in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "global info_corpus\n",
    "info_corpus = list(zip(art_clients, prices1, prices2))\n",
    "# info_corpus = info_corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_word(input_sentence: str) -> str:\n",
    "    \"\"\"Shuffle the input string\n",
    "\n",
    "    Args:\n",
    "        word (str): Input string\n",
    "\n",
    "    Returns:\n",
    "        str: Shuffled output\n",
    "    \"\"\"\n",
    "\n",
    "    words = input_sentence.split(' ')\n",
    "    random.shuffle(words)\n",
    "    return ' '.join(words)\n",
    "\n",
    "def get_items(n: int) -> list:\n",
    "    \"\"\"Get generated info from the list\n",
    "\n",
    "    Args:\n",
    "        n (int): N no of items to fetch\n",
    "\n",
    "    Returns:\n",
    "        list: return list\n",
    "    \"\"\"\n",
    "\n",
    "    if len(info_corpus) > n:\n",
    "        result = random.sample(info_corpus, n)\n",
    "        for item in result:\n",
    "            info_corpus.remove(item)\n",
    "    else:\n",
    "        result = random.sample(info_corpus, len(info_corpus))\n",
    "        for item in result:\n",
    "            info_corpus.remove(item)\n",
    "    return result\n",
    "\n",
    "def insert_entries(doc_content: str, entities_list: list) -> str:\n",
    "    \"\"\"Insert given entities into the document content\n",
    "\n",
    "    Args:\n",
    "        doc_content (str): Document content\n",
    "        entities_list (list): List of entities\n",
    "\n",
    "    Returns:\n",
    "        str: Document with the inserted entities\n",
    "    \"\"\"\n",
    "    entity_label_list = list()\n",
    "    return_entity_dict = dict()\n",
    "    doc_length = len(doc_content.split())\n",
    "    randidx = random.randint(50, int(doc_length/2))\n",
    "    \n",
    "    for artid, price1, price2 in entities_list:\n",
    "        doc_word_list = doc_content.split(' ')\n",
    "        doc_word_list.insert(randidx, artid)\n",
    "        doc_word_list.insert(randidx+random.choice([32, 48]), price1)\n",
    "        doc_word_list.insert(randidx+random.choice([64, 128]), price2)\n",
    "        doc_content = ' '.join([str(i) for i in doc_word_list])\n",
    "        randidx = randidx + random.choice([32, 64])\n",
    "    \n",
    "    for items in entities_list:\n",
    "        for item in items:\n",
    "            if '/' not in item.lower():\n",
    "                start, end = re.search(item, doc_content).span()\n",
    "                entity_label_list.append((start, end, 'Art ID'))\n",
    "                pass\n",
    "            else:\n",
    "                start, end = re.search(item, doc_content).span()\n",
    "                entity_label_list.append((start, end, 'Price per Unit'))\n",
    "    return_entity_dict['entities'] = entity_label_list\n",
    "        \n",
    "        \n",
    "    return [doc_content, return_entity_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_data(doc_content: str, data_corpus: list) -> None:\n",
    "    \"\"\"To generate synthetic data to train SpaCy custom NER model. \n",
    "    This returns None but generates a training data.\n",
    "\n",
    "    Args:\n",
    "        doc_content (str): Input document content, which will be \n",
    "                           modified.\n",
    "        data_corpus (list): Input list of items to be placed \n",
    "                            randomly in the document.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "\n",
    "    data_list = list()\n",
    "    len_doc = len(doc_content)\n",
    "\n",
    "    while len(info_corpus) > 0:\n",
    "        no_of_samples = 1 #random.randint(1, 2)\n",
    "        # new_doc_content = shuffle_word(doc_content)\n",
    "        new_doc_content = doc_content\n",
    "        entities = get_items(no_of_samples)\n",
    "        data_list.append(insert_entries(new_doc_content, entities))\n",
    "        \n",
    "    return data_list\n",
    "\n",
    "TRAINING_DATA = generate_synthetic_data(edited_content, info_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(lang: str, TRAINING_DATA, output_path: Path):\n",
    "    nlp = spacy.blank(lang)\n",
    "    db = DocBin()\n",
    "    for text, annot in TRAINING_DATA:\n",
    "        doc = nlp.make_doc(text)\n",
    "        ents = []\n",
    "        for start, end, label in annot[\"entities\"]:\n",
    "            span = doc.char_span(start, end, label=label)\n",
    "            if span is None:\n",
    "                msg = f\"Skipping entity [{start}, {end}, {label}] in the following text because the character span '{doc.text[start:end]}' does not align with token boundaries:\\n\\n{repr(text)}\\n\"\n",
    "                warnings.warn(msg)\n",
    "            else:\n",
    "                ents.append(span)\n",
    "        doc.ents = ents\n",
    "        db.add(doc)\n",
    "    db.to_disk(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert(\"fr\", TRAINING_DATA, \"/Users/gopinath.balu/Downloads/text_extraction/data/fr_model/train.spacy\")\n",
    "convert(\"fr\", TRAINING_DATA, \"/home/admin/gopi/Workspace/text_extraction/data/fr_model/train.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model 'fr_core_news_md'\n"
     ]
    }
   ],
   "source": [
    "model = 'fr_core_news_md'\n",
    "optimizer = SGD(learn_rate=0.001, L2=1e-6, grad_clip=1.0)\n",
    "losses = {}\n",
    "\n",
    "if model is not None:\n",
    "    nlp = spacy.load(model)  \n",
    "    print(\"Loaded model '%s'\" % model)\n",
    "else:\n",
    "    nlp = spacy.blank('fr')  \n",
    "    print(\"Created blank 'fr' model\")\n",
    "\n",
    "if 'ner' not in nlp.pipe_names:\n",
    "    ner = nlp.add_pipe(\"ner\")\n",
    "    ner.add_label(\"O\")\n",
    "else:\n",
    "    ner = nlp.get_pipe('ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 90.84170774878909}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 350.4345183795874}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 398.3759510827574}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 439.08778427430116}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 470.8112873588125}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 488.74916693893084}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 498.5974723671322}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 506.5360108731539}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 514.111037630536}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 520.4150380986607}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 526.6514368975318}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 532.7623874800422}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 539.0644839562742}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 545.1426919682269}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 551.1235299043822}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 686.6763316028284}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 692.6287049971781}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 698.4116826111697}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 704.219954708746}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 709.9417413933718}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 715.526472793443}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 721.2448221244651}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 726.9596062059916}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 732.8631227061462}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 738.7309626065446}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 744.6160863051475}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 750.2935671108428}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 755.3976798020724}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 760.6669176920742}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 765.7055705547056}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 770.4662953111242}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 775.4864338743175}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 779.8152540121623}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 787.0398981360286}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 791.7294033781703}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 796.3470786506389}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 801.2424638138702}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 805.7314117728384}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 810.3424950480113}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 924.8377299450154}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 978.500015309964}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 1068.9830233579942}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 1111.4487678712262}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 1204.066421402644}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 1268.541518769069}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 1323.2372563958318}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 1327.335145458197}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 1339.3856946986875}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 1348.0671360915478}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 1354.179587817999}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 1364.9019920419566}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 1370.3283687198068}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 1376.010717868604}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 1380.891751540968}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 1385.3969451806656}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 1389.8508686955797}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 1393.8258453762576}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 1397.8845643756815}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 1401.5377222199472}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 1405.2413700311845}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 1409.36549726153}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 1411.967262238951}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 1415.1726566605134}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 1418.2970146096884}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 1421.280874860637}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 1424.541945515675}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 1427.788798818845}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 1429.3729135283086}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 1431.9225417816428}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 1434.308071408388}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 1436.9615737558524}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 1439.271189340423}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 1441.3245508079892}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 1443.3451631215034}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 1444.6456088725718}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 1447.0996485665164}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 1449.0783970110506}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 1450.9546310794597}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 1451.932560377869}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 1453.423430424836}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 1454.681789205938}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 1456.7997917818086}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 1457.9915848233604}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 1459.5434344359712}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 1460.7266776004299}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 1462.367682483006}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 1464.3384828440032}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 1465.7967228831383}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 1466.9864704733395}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 1469.000547196513}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 1469.384717772626}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 1469.6056335794926}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 1469.7309402619655}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 1471.151296135636}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 1471.414505333588}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 1473.4072195805293}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 1475.246145177347}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 1475.5800916465357}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 1475.6967006425862}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 1476.6561389396356}\n"
     ]
    }
   ],
   "source": [
    "from spacy.training.example import Example\n",
    "\n",
    "for batch in spacy.util.minibatch(TRAINING_DATA, size=2):\n",
    "    for text, annotations in batch:\n",
    "        doc = nlp.make_doc(text)\n",
    "        example = Example.from_dict(doc, annotations)\n",
    "        # nlp.update([example], sgd=optimizer, losses=losses, drop=0.3)\n",
    "        nlp.update([example], losses=losses, drop=0.3)\n",
    "        print(\"Losses\", losses)\n",
    "\n",
    "# nlp.to_disk('/Users/gopinath.balu/Downloads/text_extraction/output/fr_model')\n",
    "nlp.to_disk('/home/admin/gopi/Workspace/text_extraction/output/fr_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities []\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(edited_content)\n",
    "print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a3bdab6f35d6e2d330246078e157c8a60d05073498af0570789dd7e947495cc0"
  },
  "kernelspec": {
   "display_name": "dlarith",
   "language": "python",
   "name": "dlarith"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
