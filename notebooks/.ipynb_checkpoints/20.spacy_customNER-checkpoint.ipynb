{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin/gopi/envs/dlarith/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "from tika import parser\n",
    "\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import spacy\n",
    "from thinc.api import SGD\n",
    "from spacy.tokens import DocBin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_file = '/Users/gopinath.balu/Downloads/text_extraction/data/Use caes NLP.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = parser.from_file(pdf_file)\n",
    "content = raw['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_newline = lambda x: x.replace('\\n', ' ').strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_content = remove_newline(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_content = \"POS. DESCRIPTION QUANTITE   COMMANDEE QUANTITE   CONFIRMEE PRIX /  UNITE  NET /  UNITE  MONTANT  NET  10  PFWVTG001005000803 / PWVTGG50080 Z (*) Vis à panneaux PFS+ TF-T * 5,00x80 Zn (T25)  double tête plate six lobes int. avec nervures  filet partiel - pointe coupante T17  emballage standard Art. client: 924734 QTE: 200 Pce  Demandé: 2.000 Pce Arrondi:  2.400 Pce  Stock: 2.400 Pce  49,97 / 1.000   Pce  -2 %  48,97 / 1.000   Pce  117,53   20  PFWVTV001004500603 / PWVTVK45060 Z (*) Vis à panneaux PFS+ TF-T * 4,50x60 Zn (T20)  double tête plate six lobes int. avec nervures  filet total - pointe coupante T17  emballage petit Art. client: 929735 QTE: 200 Pce  2.400 Pce Stock: 2.400 Pce  28,63 / 1.000   Pce   28,63 / 1.000   Pce  68,71   Sous Total 188,64   Remise cumulée -2,40   Montant net 186,24   TVA (21%) 39,11   Montant total 225,35  Poids brut 27,600 KG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "edited_content = \"POS. DESCRIPTION QUANTITE   COMMANDEE QUANTITE   CONFIRMEE PRIX /  UNITE  NET /  UNITE  MONTANT  NET  10  PFWVTG001005000803 / PWVTGG50080 Z (*) Vis à panneaux PFS+ TF-T * 5,00x80 Zn (T25)  double tête plate six lobes int. avec nervures  filet partiel - pointe coupante T17  emballage standard Art. client: {} QTE: 200 Pce  Demandé: 2.000 Pce Arrondi:  2.400 Pce  Stock: 2.400 Pce  {}   Pce  -2 %  {}   Pce  117,53   20  PFWVTV001004500603 / PWVTVK45060 Z (*) Vis à panneaux PFS+ TF-T * 4,50x60 Zn (T20)  double tête plate six lobes int. avec nervures  filet total - pointe coupante T17  emballage petit Art. client: {} QTE: 200 Pce  2.400 Pce Stock: 2.400 Pce  {}   Pce   {}   Pce  68,71   Sous Total 188,64   Remise cumulée -2,40   Montant net 186,24   TVA (21%) 39,11   Montant total 225,35  Poids brut 27,600 KG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#items_to_extract = ['Art. client: 924734', '49,97 €  / 1.000', '48,97 €  / 1.000', 'Art. client: 929735', '28,63 €  / 1.000', '28,63 €  / 1.000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "art_clients = [f'{random.randint(10000, 999999)}' for i in range(100)]\n",
    "prices1 = [f'{random.randint(1, 999)},{random.randint(0, 99)} / 1.000' for i in range(100)]\n",
    "prices2 = [f'{random.randint(1, 999)},{random.randint(0, 99)} / 1.000' for i in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "global info_corpus\n",
    "info_corpus = list(zip(art_clients, prices1, prices2))\n",
    "# info_corpus = info_corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_word(input_sentence: str) -> str:\n",
    "    \"\"\"Shuffle the input string\n",
    "\n",
    "    Args:\n",
    "        word (str): Input string\n",
    "\n",
    "    Returns:\n",
    "        str: Shuffled output\n",
    "    \"\"\"\n",
    "\n",
    "    words = input_sentence.split(' ')\n",
    "    random.shuffle(words)\n",
    "    return ' '.join(words)\n",
    "\n",
    "def get_items(n: int) -> list:\n",
    "    \"\"\"Get generated info from the list\n",
    "\n",
    "    Args:\n",
    "        n (int): N no of items to fetch\n",
    "\n",
    "    Returns:\n",
    "        list: return list\n",
    "    \"\"\"\n",
    "\n",
    "    if len(info_corpus) > n:\n",
    "        result = random.sample(info_corpus, n)\n",
    "        for item in result:\n",
    "            info_corpus.remove(item)\n",
    "    else:\n",
    "        result = random.sample(info_corpus, len(info_corpus))\n",
    "        for item in result:\n",
    "            info_corpus.remove(item)\n",
    "    return result\n",
    "\n",
    "def insert_entries(doc_content: str, entities_list: list) -> str:\n",
    "    \"\"\"Insert given entities into the document content\n",
    "\n",
    "    Args:\n",
    "        doc_content (str): Document content\n",
    "        entities_list (list): List of entities\n",
    "\n",
    "    Returns:\n",
    "        str: Document with the inserted entities\n",
    "    \"\"\"\n",
    "    entity_label_list = list()\n",
    "    return_entity_dict = dict()\n",
    "    # doc_length = len(doc_content.split())\n",
    "    # randidx = random.randint(50, int(doc_length/2))\n",
    "    \n",
    "    # for artid, price1, price2 in entities_list:\n",
    "    #     doc_word_list = doc_content.split(' ')\n",
    "    #     # doc_word_list.insert(randidx, artid)\n",
    "    #     doc_word_list.insert(randidx+random.choice([32, 48]), price1)\n",
    "    #     doc_word_list.insert(randidx+random.choice([64, 128]), price2)\n",
    "    #     doc_content = ' '.join([str(i) for i in doc_word_list])\n",
    "    #     randidx = randidx + random.choice([32, 64])\n",
    "    \n",
    "    merged_entities_list = entities_list[0] + entities_list[1]\n",
    "    doc_content = doc_content.format(*merged_entities_list)\n",
    "    \n",
    "    for items in entities_list:\n",
    "        for item in items:\n",
    "            if '/' not in item.lower():\n",
    "                start, end = re.search(item, doc_content).span()\n",
    "                entity_label_list.append((start, end, 'Art ID'))\n",
    "                pass\n",
    "            else:\n",
    "                start, end = re.search(item, doc_content).span()\n",
    "                entity_label_list.append((start, end, 'Price per Unit'))\n",
    "    return_entity_dict['entities'] = entity_label_list\n",
    "        \n",
    "        \n",
    "    return [doc_content, return_entity_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_data(doc_content: str, data_corpus: list) -> None:\n",
    "    \"\"\"To generate synthetic data to train SpaCy custom NER model. \n",
    "    This returns None but generates a training data.\n",
    "\n",
    "    Args:\n",
    "        doc_content (str): Input document content, which will be \n",
    "                           modified.\n",
    "        data_corpus (list): Input list of items to be placed \n",
    "                            randomly in the document.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "\n",
    "    data_list = list()\n",
    "    len_doc = len(doc_content)\n",
    "\n",
    "    while len(info_corpus) > 0:\n",
    "        no_of_samples = 2 #random.randint(1, 2)\n",
    "        # new_doc_content = shuffle_word(doc_content)\n",
    "        new_doc_content = doc_content\n",
    "        entities = get_items(no_of_samples)\n",
    "        data_list.append(insert_entries(new_doc_content, entities))\n",
    "        \n",
    "    return data_list\n",
    "\n",
    "TRAINING_DATA = generate_synthetic_data(edited_content, info_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(lang: str, TRAINING_DATA, output_path: Path):\n",
    "    nlp = spacy.blank(lang)\n",
    "    db = DocBin()\n",
    "    for text, annot in TRAINING_DATA:\n",
    "        doc = nlp.make_doc(text)\n",
    "        ents = []\n",
    "        for start, end, label in annot[\"entities\"]:\n",
    "            span = doc.char_span(start, end, label=label)\n",
    "            if span is None:\n",
    "                msg = f\"Skipping entity [{start}, {end}, {label}] in the following text because the character span '{doc.text[start:end]}' does not align with token boundaries:\\n\\n{repr(text)}\\n\"\n",
    "                warnings.warn(msg)\n",
    "            else:\n",
    "                ents.append(span)\n",
    "        doc.ents = ents\n",
    "        db.add(doc)\n",
    "    db.to_disk(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert(\"fr\", TRAINING_DATA, \"/home/admin/gopi/Workspace/text_extraction/data/fr_model2/train.spacy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model 'fr_core_news_md'\n"
     ]
    }
   ],
   "source": [
    "model = 'fr_core_news_md'\n",
    "optimizer = SGD(learn_rate=0.001, L2=1e-6, grad_clip=1.0)\n",
    "losses = {}\n",
    "\n",
    "if model is not None:\n",
    "    nlp = spacy.load(model)  \n",
    "    print(\"Loaded model '%s'\" % model)\n",
    "else:\n",
    "    nlp = spacy.blank('fr')  \n",
    "    print(\"Created blank 'fr' model\")\n",
    "\n",
    "if 'ner' not in nlp.pipe_names:\n",
    "    ner = nlp.add_pipe(\"ner\")\n",
    "    ner.add_label(\"O\")\n",
    "else:\n",
    "    ner = nlp.get_pipe('ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 49.524407568274036}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 92.9774876906314}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 129.8575642896357}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 152.87631826737461}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 172.99090532587257}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 189.06375478374707}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 202.5538351176792}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 215.44947408671666}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 226.91382889794565}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 238.6083827823296}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 249.6635642103138}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 261.37816370099245}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 272.1246694123819}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 282.4499701796939}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 292.8725957407297}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 330.6043876972043}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 338.61912114484574}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 345.8615533729436}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 354.91635310295817}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 361.582244353329}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 368.6266306308935}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 374.949877556796}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 424.05360399762316}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 453.9117054725549}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 468.1788255357066}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 478.43230630198394}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 494.09797425658087}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 520.6221163294333}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 553.0282965610938}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 562.6717497439802}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 587.2432521322946}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 600.1517711714039}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 621.936826723726}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 650.3088802481783}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 685.2860545241108}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 715.6883980347927}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 734.3421114273149}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 746.3163056895455}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 751.9530679465653}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 758.191689846903}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 765.7679264869074}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 771.661228296496}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 779.6698105918338}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 786.9997518797968}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 794.1221073719902}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 801.1973371929548}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 808.029882001568}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 814.0981749165126}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 819.5588592279062}\n",
      "Losses {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 824.6497302413095}\n"
     ]
    }
   ],
   "source": [
    "from spacy.training.example import Example\n",
    "\n",
    "for batch in spacy.util.minibatch(TRAINING_DATA, size=2):\n",
    "    for text, annotations in batch:\n",
    "        doc = nlp.make_doc(text)\n",
    "        example = Example.from_dict(doc, annotations)\n",
    "        # nlp.update([example], sgd=optimizer, losses=losses, drop=0.3)\n",
    "        nlp.update([example], losses=losses, drop=0.3)\n",
    "        print(\"Losses\", losses)\n",
    "\n",
    "nlp.to_disk('/home/admin/gopi/Workspace/text_extraction/output/fr_model2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities [('924734', 'Art ID'), ('49,97 / 1.000', 'Price per Unit'), ('48,97 / 1.000', 'Price per Unit'), ('929735', 'Art ID'), ('28,63 / 1.000', 'Price per Unit'), ('28,63', 'Art ID')]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(actual_content)\n",
    "print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a3bdab6f35d6e2d330246078e157c8a60d05073498af0570789dd7e947495cc0"
  },
  "kernelspec": {
   "display_name": "dlarith",
   "language": "python",
   "name": "dlarith"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
