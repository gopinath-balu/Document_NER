{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "from tika import parser, translate\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from pathlib import Path\n",
    "\n",
    "import spacy\n",
    "from thinc.api import SGD, Adam\n",
    "from spacy.tokens import DocBin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf_file = '/Users/gopinath.balu/Downloads/text_extraction/data/Use caes NLP.pdf'\n",
    "pdf_file = '/home/admin/gopi/Workspace/text_extraction/data/Use caes NLP.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw = parser.from_file(pdf_file)\n",
    "# content = raw['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_newline = lambda x: x.replace('\\n', ' ').strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual_content = remove_newline(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_content = \"POS. DESCRIPTION QUANTITE   COMMANDEE QUANTITE   CONFIRMEE PRIX /  UNITE  NET /  UNITE  MONTANT  NET  10  PFWVTG001005000803 / PWVTGG50080 Z (*) Vis à panneaux PFS+ TF-T * 5,00x80 Zn (T25)  double tête plate six lobes int. avec nervures  filet partiel - pointe coupante T17  emballage standard Art. client: 924734 QTE: 200 Pce  Demandé: 2.000 Pce Arrondi:  2.400 Pce  Stock: 2.400 Pce  49,97 / 1.000   Pce  -2 %  48,97 / 1.000   Pce  117,53   20  PFWVTV001004500603 / PWVTVK45060 Z (*) Vis à panneaux PFS+ TF-T * 4,50x60 Zn (T20)  double tête plate six lobes int. avec nervures  filet total - pointe coupante T17  emballage petit Art. client: 929735 QTE: 200 Pce  2.400 Pce Stock: 2.400 Pce  28,63 / 1.000   Pce   28,63 / 1.000   Pce  68,71   Sous Total 188,64   Remise cumulée -2,40   Montant net 186,24   TVA (21%) 39,11   Montant total 225,35  Poids brut 27,600 KG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_content_fr = translate.from_buffer(actual_content, 'fr', 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "edited_content = \"POS. DESCRIPTION QUANTITY ORDERED QUANTITY CONFIRMED PRICE / UNIT NET / UNIT NET AMOUNT 10 PFWVTG001005000803 / PWVTGG50080 Z (*) Panel screw PFS+ TF-T * 5.00x80 Zn (T25) double flat head six lobes int. with ribs partial thread - cutting tip T17 standard packaging Art. customer: {} QTY: 200 Pce Requested: 2,000 Pce Rounded: 2,400 Pce Stock: 2,400 Pce {} Pce -2 % {} Pce 117.53 20 PFWVTV001004500603 / PWVTVK45060 PFS (*) Panel screws -T * 4.50x60 Zn (T20) double flat head six lobe int. with full thread ribs - cutting tip T17 small packaging Art. customer: {} QTY: 200 pcs 2,400 pcs Stock: 2,400 pcs {} pcs {} pcs 68.71 Subtotal 188.64 Cumulative discount -2.40 Net amount 186.24 VAT (21%) 39 .11 Total amount 225.35 Gross weight 27,600 KG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#items_to_extract = ['Art. client: 924734', '49,97 €  / 1.000', '48,97 €  / 1.000', 'Art. client: 929735', '28,63 €  / 1.000', '28,63 €  / 1.000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "art_clients = [f'{random.randint(10000, 999999)}' for i in range(100)]\n",
    "prices1 = [f'{random.randint(1, 999)},{random.randint(0, 99)} / 1.000' for i in range(100)]\n",
    "prices2 = [f'{random.randint(1, 999)},{random.randint(0, 99)} / 1.000' for i in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "global info_corpus\n",
    "info_corpus = list(zip(art_clients, prices1, prices2))\n",
    "# info_corpus = info_corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_word(input_sentence: str) -> str:\n",
    "    \"\"\"Shuffle the input string\n",
    "\n",
    "    Args:\n",
    "        word (str): Input string\n",
    "\n",
    "    Returns:\n",
    "        str: Shuffled output\n",
    "    \"\"\"\n",
    "\n",
    "    words = input_sentence.split(' ')\n",
    "    random.shuffle(words)\n",
    "    return ' '.join(words)\n",
    "\n",
    "def get_items(n: int) -> list:\n",
    "    \"\"\"Get generated info from the list\n",
    "\n",
    "    Args:\n",
    "        n (int): N no of items to fetch\n",
    "\n",
    "    Returns:\n",
    "        list: return list\n",
    "    \"\"\"\n",
    "\n",
    "    if len(info_corpus) > n:\n",
    "        result = random.sample(info_corpus, n)\n",
    "        for item in result:\n",
    "            info_corpus.remove(item)\n",
    "    else:\n",
    "        result = random.sample(info_corpus, len(info_corpus))\n",
    "        for item in result:\n",
    "            info_corpus.remove(item)\n",
    "    return result\n",
    "\n",
    "def insert_entries(doc_content: str, entities_list: list) -> str:\n",
    "    \"\"\"Insert given entities into the document content\n",
    "\n",
    "    Args:\n",
    "        doc_content (str): Document content\n",
    "        entities_list (list): List of entities\n",
    "\n",
    "    Returns:\n",
    "        str: Document with the inserted entities\n",
    "    \"\"\"\n",
    "    entity_label_list = list()\n",
    "    return_entity_dict = dict()\n",
    "    # doc_length = len(doc_content.split())\n",
    "    # randidx = random.randint(50, int(doc_length/2))\n",
    "    \n",
    "    # for artid, price1, price2 in entities_list:\n",
    "    #     doc_word_list = doc_content.split(' ')\n",
    "    #     # doc_word_list.insert(randidx, artid)\n",
    "    #     doc_word_list.insert(randidx+random.choice([32, 48]), price1)\n",
    "    #     doc_word_list.insert(randidx+random.choice([64, 128]), price2)\n",
    "    #     doc_content = ' '.join([str(i) for i in doc_word_list])\n",
    "    #     randidx = randidx + random.choice([32, 64])\n",
    "    \n",
    "    merged_entities_list = entities_list[0] + entities_list[1]\n",
    "    doc_content = doc_content.format(*merged_entities_list)\n",
    "    \n",
    "    for items in entities_list:\n",
    "        for item in items:\n",
    "            if '/' not in item.lower():\n",
    "                start, end = re.search(item, doc_content).span()\n",
    "                entity_label_list.append((start, end, 'Art ID'))\n",
    "                pass\n",
    "            else:\n",
    "                start, end = re.search(item, doc_content).span()\n",
    "                entity_label_list.append((start, end, 'Price per Unit'))\n",
    "    return_entity_dict['entities'] = entity_label_list\n",
    "        \n",
    "        \n",
    "    return [doc_content, return_entity_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_data(doc_content: str, data_corpus: list) -> None:\n",
    "    \"\"\"To generate synthetic data to train SpaCy custom NER model. \n",
    "    This returns None but generates a training data.\n",
    "\n",
    "    Args:\n",
    "        doc_content (str): Input document content, which will be \n",
    "                           modified.\n",
    "        data_corpus (list): Input list of items to be placed \n",
    "                            randomly in the document.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "\n",
    "    data_list = list()\n",
    "    len_doc = len(doc_content)\n",
    "\n",
    "    while len(info_corpus) > 0:\n",
    "        no_of_samples = 2 #random.randint(1, 2)\n",
    "        # new_doc_content = shuffle_word(doc_content)\n",
    "        new_doc_content = doc_content\n",
    "        entities = get_items(no_of_samples)\n",
    "        data_list.append(insert_entries(new_doc_content, entities))\n",
    "        \n",
    "    return data_list\n",
    "\n",
    "TRAINING_DATA = generate_synthetic_data(edited_content, info_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(lang: str, TRAINING_DATA, output_path: Path):\n",
    "    nlp = spacy.blank(lang)\n",
    "    db = DocBin()\n",
    "    for text, annot in TRAINING_DATA:\n",
    "        doc = nlp.make_doc(text)\n",
    "        ents = []\n",
    "        for start, end, label in annot[\"entities\"]:\n",
    "            span = doc.char_span(start, end, label=label)\n",
    "            if span is None:\n",
    "                msg = f\"Skipping entity [{start}, {end}, {label}] in the following text because the character span '{doc.text[start:end]}' does not align with token boundaries:\\n\\n{repr(text)}\\n\"\n",
    "                warnings.warn(msg)\n",
    "            else:\n",
    "                ents.append(span)\n",
    "        doc.ents = ents\n",
    "        db.add(doc)\n",
    "    db.to_disk(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert(\"en\", TRAINING_DATA, \"/Users/gopinath.balu/Downloads/text_extraction/data/eng_transformers/train.spacy\")\n",
    "convert(\"en\", TRAINING_DATA, \"/home/admin/gopi/Workspace/text_extraction/data/eng_transformers2/train.spacy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model 'en_core_web_trf'\n"
     ]
    }
   ],
   "source": [
    "model = 'en_core_web_trf'\n",
    "optimizer = SGD(learn_rate=0.001, L2=1e-6, grad_clip=1.0)\n",
    "losses = {}\n",
    "\n",
    "if model is not None:\n",
    "    nlp = spacy.load(model)  \n",
    "    print(\"Loaded model '%s'\" % model)\n",
    "else:\n",
    "    nlp = spacy.blank('en')  \n",
    "    print(\"Created blank 'EN' model\")\n",
    "\n",
    "if 'ner' not in nlp.pipe_names:\n",
    "    ner = nlp.add_pipe(\"ner\")\n",
    "    ner.add_label(\"O\")\n",
    "else:\n",
    "    ner = nlp.get_pipe('ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'transformer': 34.92538833618164, 'tagger': 0.0, 'parser': 0.0, 'ner': 40.44785627013988}\n",
      "Losses {'transformer': 69.10708999633789, 'tagger': 0.0, 'parser': 0.0, 'ner': 79.45918650922442}\n",
      "Losses {'transformer': 105.32865524291992, 'tagger': 0.0, 'parser': 0.0, 'ner': 119.86993338582877}\n",
      "Losses {'transformer': 139.03185653686523, 'tagger': 0.0, 'parser': 0.0, 'ner': 157.8640134526745}\n",
      "Losses {'transformer': 178.9457664489746, 'tagger': 0.0, 'parser': 0.0, 'ner': 198.99368768016032}\n",
      "Losses {'transformer': 210.9647445678711, 'tagger': 0.0, 'parser': 0.0, 'ner': 237.8033992141253}\n",
      "Losses {'transformer': 246.0217514038086, 'tagger': 0.0, 'parser': 0.0, 'ner': 277.6414499656164}\n",
      "Losses {'transformer': 279.7715301513672, 'tagger': 0.0, 'parser': 0.0, 'ner': 314.26056814189457}\n",
      "Losses {'transformer': 314.91211318969727, 'tagger': 0.0, 'parser': 0.0, 'ner': 353.03792506053884}\n",
      "Losses {'transformer': 349.9650459289551, 'tagger': 0.0, 'parser': 0.0, 'ner': 391.5449632551118}\n",
      "Losses {'transformer': 386.3224678039551, 'tagger': 0.0, 'parser': 0.0, 'ner': 431.40308344074816}\n",
      "Losses {'transformer': 890.2427864074707, 'tagger': 0.0, 'parser': 0.0, 'ner': 484.86795393764595}\n",
      "Losses {'transformer': 923.3419189453125, 'tagger': 0.0, 'parser': 0.0, 'ner': 521.8449138559431}\n",
      "Losses {'transformer': 957.7590942382812, 'tagger': 0.0, 'parser': 0.0, 'ner': 561.3736323434412}\n",
      "Losses {'transformer': 992.4457740783691, 'tagger': 0.0, 'parser': 0.0, 'ner': 598.6443399076678}\n",
      "Losses {'transformer': 1028.337287902832, 'tagger': 0.0, 'parser': 0.0, 'ner': 638.9319897103907}\n",
      "Losses {'transformer': 2108.923225402832, 'tagger': 0.0, 'parser': 0.0, 'ner': 702.3303931793592}\n",
      "Losses {'transformer': 2143.631938934326, 'tagger': 0.0, 'parser': 0.0, 'ner': 741.2954959975687}\n",
      "Losses {'transformer': 2182.3258476257324, 'tagger': 0.0, 'parser': 0.0, 'ner': 781.6245435893508}\n",
      "Losses {'transformer': 2213.3720111846924, 'tagger': 0.0, 'parser': 0.0, 'ner': 816.0493316190157}\n",
      "Losses {'transformer': 2244.0186252593994, 'tagger': 0.0, 'parser': 0.0, 'ner': 850.5718246469805}\n",
      "Losses {'transformer': 2278.217260360718, 'tagger': 0.0, 'parser': 0.0, 'ner': 889.5803621050043}\n",
      "Losses {'transformer': 2313.776300430298, 'tagger': 0.0, 'parser': 0.0, 'ner': 930.6022169874835}\n",
      "Losses {'transformer': 2349.366704940796, 'tagger': 0.0, 'parser': 0.0, 'ner': 970.3911872115801}\n",
      "Losses {'transformer': 2385.6967639923096, 'tagger': 0.0, 'parser': 0.0, 'ner': 1011.1786197672083}\n",
      "Losses {'transformer': 2423.7557735443115, 'tagger': 0.0, 'parser': 0.0, 'ner': 1053.8550561717634}\n",
      "Losses {'transformer': 2471.1100368499756, 'tagger': 0.0, 'parser': 0.0, 'ner': 1094.9656836293225}\n",
      "Losses {'transformer': 2506.8242511749268, 'tagger': 0.0, 'parser': 0.0, 'ner': 1138.0677678293675}\n",
      "Losses {'transformer': 2543.8523120880127, 'tagger': 0.0, 'parser': 0.0, 'ner': 1179.20373401493}\n",
      "Losses {'transformer': 2576.13938331604, 'tagger': 0.0, 'parser': 0.0, 'ner': 1214.768087679692}\n",
      "Losses {'transformer': 2611.4863452911377, 'tagger': 0.0, 'parser': 0.0, 'ner': 1254.5803117620378}\n",
      "Losses {'transformer': 2648.6624851226807, 'tagger': 0.0, 'parser': 0.0, 'ner': 1296.3657001253873}\n",
      "Losses {'transformer': 2685.214906692505, 'tagger': 0.0, 'parser': 0.0, 'ner': 1336.5916765109193}\n",
      "Losses {'transformer': 2717.006483078003, 'tagger': 0.0, 'parser': 0.0, 'ner': 1372.448860274557}\n",
      "Losses {'transformer': 2746.4323768615723, 'tagger': 0.0, 'parser': 0.0, 'ner': 1405.903258040287}\n",
      "Losses {'transformer': 2778.9326934814453, 'tagger': 0.0, 'parser': 0.0, 'ner': 1443.544840512641}\n",
      "Losses {'transformer': 2814.889961242676, 'tagger': 0.0, 'parser': 0.0, 'ner': 1480.929162960612}\n",
      "Losses {'transformer': 2849.406520843506, 'tagger': 0.0, 'parser': 0.0, 'ner': 1520.9487610642202}\n",
      "Losses {'transformer': 2901.2205390930176, 'tagger': 0.0, 'parser': 0.0, 'ner': 1561.8580011034314}\n",
      "Losses {'transformer': 2935.366470336914, 'tagger': 0.0, 'parser': 0.0, 'ner': 1599.798139537638}\n",
      "Losses {'transformer': 2966.878215789795, 'tagger': 0.0, 'parser': 0.0, 'ner': 1635.7710425415403}\n",
      "Losses {'transformer': 2999.5166511535645, 'tagger': 0.0, 'parser': 0.0, 'ner': 1671.9116200536319}\n",
      "Losses {'transformer': 3035.6214599609375, 'tagger': 0.0, 'parser': 0.0, 'ner': 1710.7311453558295}\n",
      "Losses {'transformer': 3075.7606468200684, 'tagger': 0.0, 'parser': 0.0, 'ner': 1754.5466342818522}\n",
      "Losses {'transformer': 3110.076099395752, 'tagger': 0.0, 'parser': 0.0, 'ner': 1793.2329093656415}\n",
      "Losses {'transformer': 3147.671886444092, 'tagger': 0.0, 'parser': 0.0, 'ner': 1834.5125539531318}\n",
      "Losses {'transformer': 3181.3383255004883, 'tagger': 0.0, 'parser': 0.0, 'ner': 1870.9474341589575}\n",
      "Losses {'transformer': 3213.291904449463, 'tagger': 0.0, 'parser': 0.0, 'ner': 1907.0833969434093}\n",
      "Losses {'transformer': 3247.470920562744, 'tagger': 0.0, 'parser': 0.0, 'ner': 1944.4555981554042}\n"
     ]
    }
   ],
   "source": [
    "from spacy.training.example import Example\n",
    "\n",
    "for batch in spacy.util.minibatch(TRAINING_DATA, size=2):\n",
    "    for text, annotations in batch:\n",
    "        doc = nlp.make_doc(text)\n",
    "        example = Example.from_dict(doc, annotations)\n",
    "        # nlp.update([example], sgd=optimizer, losses=losses, drop=0.3)\n",
    "        nlp.update([example], losses=losses, drop=0.3)\n",
    "        print(\"Losses\", losses)\n",
    "\n",
    "nlp.to_disk('/home/admin/gopi/Workspace/text_extraction/output/eng_transformers2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities [('six', 'CARDINAL'), ('six', 'CARDINAL'), ('929735', 'CARDINAL'), ('200', 'CARDINAL'), ('2.400', 'CARDINAL'), ('28,63', 'CARDINAL'), ('28,63', 'CARDINAL'), ('186,24   TVA', 'PERCENT'), ('21%', 'PERCENT'), ('39,11', 'CARDINAL'), ('Montant', 'ORG')]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(actual_content)\n",
    "print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a3bdab6f35d6e2d330246078e157c8a60d05073498af0570789dd7e947495cc0"
  },
  "kernelspec": {
   "display_name": "dlarith",
   "language": "python",
   "name": "dlarith"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
